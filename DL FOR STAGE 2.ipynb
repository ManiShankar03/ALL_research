{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a7ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/mehradaria/leukemia?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 110M/110M [00:11<00:00, 10.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /Users/manishankar/.cache/kagglehub/datasets/mehradaria/leukemia/versions/1\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/manishankar/.cache/kagglehub/datasets/mehradaria/leukemia/versions/1/leukemia/Benign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m dataset \u001b[38;5;241m=\u001b[39m LeukemiaDataset(data_dir, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     66\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m     67\u001b[0m test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size\n",
      "Cell \u001b[0;32mIn[3], line 50\u001b[0m, in \u001b[0;36mLeukemiaDataset.__init__\u001b[0;34m(self, root_dir, transform)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses:\n\u001b[1;32m     49\u001b[0m     class_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, label)\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(class_path):\n\u001b[1;32m     51\u001b[0m         img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_path, img_name)\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mappend((img_path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mindex(label)))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/manishankar/.cache/kagglehub/datasets/mehradaria/leukemia/versions/1/leukemia/Benign'"
     ]
    }
   ],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import kagglehub\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import read_image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from PIL import Image\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "# Load dataset path\n",
    "path = kagglehub.dataset_download(\"mehradaria/leukemia\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "data_dir = os.path.join(path, \"leukemia\")\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Custom dataset class\n",
    "class LeukemiaDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['Benign', 'Early', 'Pre', 'Pro']\n",
    "        self.data = []\n",
    "        for label in self.classes:\n",
    "            class_path = os.path.join(root_dir, label)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                self.data.append((img_path, self.classes.index(label)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load dataset\n",
    "dataset = LeukemiaDataset(data_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=['Benign', 'Early', 'Pre', 'Pro']))\n",
    "\n",
    "# Train and evaluate\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3868e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m709.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (4.66.4)\n",
      "Requirement already satisfied: python-slugify in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: bleach in /opt/anaconda3/lib/python3.12/site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from bleach->kaggle) (24.1)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/anaconda3/lib/python3.12/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kaggle) (3.10)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105786 sha256=79cdf5b45c69414ecbf96c67bf6e79ceb016e22f5a5c1a9fea3ffffbfeb14eb4\n",
      "  Stored in directory: /Users/manishankar/Library/Caches/pip/wheels/46/d2/26/84d0a1acdb9c6baccf7d28cf06962ec80529fe1ad938489983\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.6.17\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b9a920",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kaggle.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/shutil.py:847\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 847\u001b[0m     os\u001b[38;5;241m.\u001b[39mrename(src, real_dst)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kaggle.json' -> '/Users/manishankar/.kaggle/kaggle.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(kaggle_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Move kaggle.json to ~/.kaggle directory\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m shutil\u001b[38;5;241m.\u001b[39mmove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaggle.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(kaggle_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaggle.json\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Set permissions to prevent access issues\u001b[39;00m\n\u001b[1;32m     12\u001b[0m os\u001b[38;5;241m.\u001b[39mchmod(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(kaggle_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaggle.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m600\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/shutil.py:867\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    865\u001b[0m         rmtree(src)\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 867\u001b[0m         copy_function(src, real_dst)\n\u001b[1;32m    868\u001b[0m         os\u001b[38;5;241m.\u001b[39munlink(src)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/shutil.py:475\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m copyfile(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    476\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/shutil.py:260\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    258\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kaggle.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the path for Kaggle API credentials\n",
    "kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "\n",
    "# Move kaggle.json to ~/.kaggle directory\n",
    "shutil.move(\"kaggle.json\", os.path.join(kaggle_dir, \"kaggle.json\"))\n",
    "\n",
    "# Set permissions to prevent access issues\n",
    "os.chmod(os.path.join(kaggle_dir, \"kaggle.json\"), 600)\n",
    "\n",
    "print(\"✅ kaggle.json has been moved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea91df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/manishankar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "current_dir = os.getcwd() \n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91cc83d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/manishankar/kaggle.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/shutil.py:847\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 847\u001b[0m     os\u001b[38;5;241m.\u001b[39mrename(src, real_dst)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/manishankar/kaggle.json' -> '/Users/manishankar/.kaggle/kaggle.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(destination_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Move the file\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m shutil\u001b[38;5;241m.\u001b[39mmove(source_path, destination_path)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Set permissions\u001b[39;00m\n\u001b[1;32m     16\u001b[0m os\u001b[38;5;241m.\u001b[39mchmod(destination_path, \u001b[38;5;241m600\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/shutil.py:867\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    865\u001b[0m         rmtree(src)\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 867\u001b[0m         copy_function(src, real_dst)\n\u001b[1;32m    868\u001b[0m         os\u001b[38;5;241m.\u001b[39munlink(src)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/shutil.py:475\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m copyfile(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    476\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/shutil.py:260\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    258\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/manishankar/kaggle.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source and destination paths\n",
    "source_path = \"/Users/manishankar/kaggle.json\"\n",
    "destination_dir = os.path.expanduser(\"~/.kaggle\")\n",
    "destination_path = os.path.join(destination_dir, \"kaggle.json\")\n",
    "\n",
    "# Create ~/.kaggle directory if it doesn't exist\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Move the file\n",
    "shutil.move(source_path, destination_path)\n",
    "\n",
    "# Set permissions\n",
    "os.chmod(destination_path, 600)\n",
    "\n",
    "print(\"✅ kaggle.json has been moved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96db59bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Directory 'all_dataset' created at: /Users/manishankar/all_dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path for the new directory\n",
    "directory_path = \"/Users/manishankar/all_dataset\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Directory 'all_dataset' created at: {directory_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ef0b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ kaggle.json has been moved successfully to ~/.kaggle/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source and destination paths\n",
    "source_path = \"/Users/manishankar/all_dataset/kaggle.json\"  # Correct location of your file\n",
    "destination_dir = os.path.expanduser(\"~/.kaggle\")  # Kaggle API directory\n",
    "destination_path = os.path.join(destination_dir, \"kaggle.json\")\n",
    "\n",
    "# Create ~/.kaggle directory if it doesn't exist\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Move the file to the correct location\n",
    "shutil.move(source_path, destination_path)\n",
    "\n",
    "# Set permissions\n",
    "os.chmod(destination_path, 600)\n",
    "\n",
    "print(\"✅ kaggle.json has been moved successfully to ~/.kaggle/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09f564ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current permissions for kaggle.json: 130\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"/Users/manishankar/.kaggle/kaggle.json\"\n",
    "permissions = oct(os.stat(file_path).st_mode)[-3:]\n",
    "print(f\"Current permissions for kaggle.json: {permissions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e127dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.3962\n",
      "Epoch 2, Loss: 0.3274\n",
      "Epoch 3, Loss: 0.1824\n",
      "Epoch 4, Loss: 0.1271\n",
      "Epoch 5, Loss: 0.1077\n",
      "Epoch 6, Loss: 0.0981\n",
      "Epoch 7, Loss: 0.0846\n",
      "Epoch 8, Loss: 0.0996\n",
      "Epoch 9, Loss: 0.1351\n",
      "Epoch 10, Loss: 0.0788\n",
      "Accuracy: 0.9877300613496932\n",
      "F1 Score: 0.9877304541097388\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.97      0.97      0.97       116\n",
      "       Early       0.99      0.99      0.99       203\n",
      "         Pre       1.00      0.98      0.99       182\n",
      "         Pro       0.99      1.00      0.99       151\n",
      "\n",
      "    accuracy                           0.99       652\n",
      "   macro avg       0.99      0.99      0.99       652\n",
      "weighted avg       0.99      0.99      0.99       652\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/manishankar/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████████████████████████████████| 97.8M/97.8M [01:09<00:00, 1.47MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet-50...\n",
      "Epoch 1, Loss: 0.2407\n",
      "Epoch 2, Loss: 0.0942\n",
      "Epoch 3, Loss: 0.0829\n",
      "Epoch 4, Loss: 0.0299\n",
      "Epoch 5, Loss: 0.0372\n",
      "Epoch 6, Loss: 0.0429\n",
      "Epoch 7, Loss: 0.0251\n",
      "Epoch 8, Loss: 0.0173\n",
      "Epoch 9, Loss: 0.0203\n",
      "Epoch 10, Loss: 0.0189\n",
      "Evaluating ResNet-50...\n",
      "Accuracy: 0.9969325153374233\n",
      "F1 Score: 0.9969416181444438\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.98      1.00      0.99       116\n",
      "       Early       1.00      1.00      1.00       203\n",
      "         Pre       1.00      0.99      1.00       182\n",
      "         Pro       1.00      1.00      1.00       151\n",
      "\n",
      "    accuracy                           1.00       652\n",
      "   macro avg       1.00      1.00      1.00       652\n",
      "weighted avg       1.00      1.00      1.00       652\n",
      "\n",
      "Extracting deep features for SVM...\n",
      "SVM Accuracy: 1.0\n",
      "SVM F1 Score: 1.0\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Benign       1.00      1.00      1.00       116\n",
      "       Early       1.00      1.00      1.00       203\n",
      "         Pre       1.00      1.00      1.00       182\n",
      "         Pro       1.00      1.00      1.00       151\n",
      "\n",
      "    accuracy                           1.00       652\n",
      "   macro avg       1.00      1.00      1.00       652\n",
      "weighted avg       1.00      1.00      1.00       652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# =============================\n",
    "# 1. Set up Reproducibility\n",
    "# =============================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# =============================\n",
    "# 2. Define Dataset Paths\n",
    "# =============================\n",
    "dataset_path = \"/Users/manishankar/all_dataset/Original\"\n",
    "classes = [\"Benign\", \"Early\", \"Pre\", \"Pro\"]\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# =============================\n",
    "# 3. Custom Dataset Class\n",
    "# =============================\n",
    "class LeukemiaDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        \n",
    "        for label in classes:\n",
    "            class_path = os.path.join(root_dir, label)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                self.data.append((img_path, classes.index(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# =============================\n",
    "# 4. Load Data\n",
    "# =============================\n",
    "dataset = LeukemiaDataset(dataset_path, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# =============================\n",
    "# 5. Improved CNN Model\n",
    "# =============================\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 4)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# =============================\n",
    "# 6. Train and Evaluate CNN\n",
    "# =============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn_model = ImprovedCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=classes))\n",
    "\n",
    "# Train and Evaluate CNN\n",
    "train_model(cnn_model, train_loader, criterion, optimizer, epochs=10)\n",
    "evaluate_model(cnn_model, test_loader)\n",
    "\n",
    "# =============================\n",
    "# 7. ResNet-50 Model\n",
    "# =============================\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 4)  # Adjust output for 4 classes\n",
    "resnet_model = resnet_model.to(device)\n",
    "\n",
    "optimizer_resnet = optim.Adam(resnet_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and Evaluate ResNet-50\n",
    "print(\"Training ResNet-50...\")\n",
    "train_model(resnet_model, train_loader, criterion, optimizer_resnet, epochs=10)\n",
    "print(\"Evaluating ResNet-50...\")\n",
    "evaluate_model(resnet_model, test_loader)\n",
    "\n",
    "# =============================\n",
    "# 8. SVM Classifier using Deep Features\n",
    "# =============================\n",
    "def extract_features(model, data_loader):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, batch_labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            labels.extend(batch_labels.numpy())\n",
    "    \n",
    "    features = np.vstack(features)\n",
    "    return features, np.array(labels)\n",
    "\n",
    "# Extract deep features using ResNet\n",
    "print(\"Extracting deep features for SVM...\")\n",
    "train_features, train_labels = extract_features(resnet_model, train_loader)\n",
    "test_features, test_labels = extract_features(resnet_model, test_loader)\n",
    "\n",
    "# Train SVM on extracted features\n",
    "svm_model = SVC(kernel='linear', C=1.0)\n",
    "svm_model.fit(train_features, train_labels)\n",
    "\n",
    "# Evaluate SVM\n",
    "svm_predictions = svm_model.predict(test_features)\n",
    "print(\"SVM Accuracy:\", accuracy_score(test_labels, svm_predictions))\n",
    "print(\"SVM F1 Score:\", f1_score(test_labels, svm_predictions, average='weighted'))\n",
    "print(\"SVM Classification Report:\\n\", classification_report(test_labels, svm_predictions, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8126ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
